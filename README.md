
# **Inclusive Meeting Assistant** ğŸ™ğŸ¤

An AI-powered assistant that makes meetings more **accessible and inclusive**, inspired by [Read.ai](https://read.ai), with added **sign language support** for differentially abled participants.  
This project integrates **speech recognition, sign language translation, summarization, action item extraction, translation, text-to-speech, email export, and speaker diarization** into a unified pipeline.

---

## **âœ¨ Features Implemented**

### **1. ğŸ¤ Speech to Text (ASR)**
- Converts meeting audio into text using **OpenAI Whisper** or ASR pipeline.
- **Input:** Recorded audio from `speech_Module`  
- **Output:** `output/transcript.txt`

### **2. ğŸ– Sign Language Detection**

#### **Phase 4: ML-Based Recognition (INTEGRATED)**
- **Real-time sign language recognition** with **TensorFlow LSTM model**
- Detects **6 basic meeting gestures**: hello, yes, no, question, thanks, idle
- **>80% confidence threshold** for accurate detection
- **Fully integrated with live meeting chat** - signs automatically appear in transcript feed
- **WebSocket broadcasting** - all participants see sign language messages in real-time
- Special UI styling with gradient backgrounds and emojis
- Camera overlay shows detected signs with confidence levels
- See [SIGN_LANGUAGE_INTEGRATION.md](SIGN_LANGUAGE_INTEGRATION.md) for complete guide

#### **Phase 5: Client-Side Browser Detection**
- **Browser-based ASL detection** using **Google MediaPipe**
- Detects hand landmarks and recognizes gestures using geometry-based algorithms
- Supports **10 letters** (A, B, C, D, F, I, L, O, V, Y) and **5 numbers** (1-5)
- **Client-side processing** - no video sent to server
- Live webcam feed with hand landmark visualization
- Text accumulation, copy, download, and history tracking
- **GPU accelerated** with 25-30 FPS performance
- See [PHASE5_SUMMARY.md](PHASE5_SUMMARY.md) for details

### **3. ğŸ“ Meeting Summarization**
- Summarizes long transcripts into concise notes
- Powered by **HuggingFace Transformers** (`distilbart-cnn-12-6`)
- Example: 10-minute transcript â†’ 4â€“5 sentence summary

### **4. âœ… Action Items Extraction**
- Extracts decisions, todos, and next steps from meeting notes
- Powered by **Google FLAN-T5**
- Returns **bullet-point action items**

### **5. ğŸŒ Multilingual Translation**
- Translates transcripts or summaries into multiple languages
- Uses **Helsinki-NLP MarianMT models**
- Example: `en â†’ hi`, `en â†’ fr`, etc.

### **6. ğŸ”Š Text to Speech (TTS)**
- Converts meeting summaries or action items into speech audio
- Uses **pyttsx3** for offline TTS
- **Output:** `.wav` audio file for recap

### **7. ğŸ“§ Email Export**
- Exports meeting notes & action items to email
- SMTP integration (tested; DNS issues pending fix)
- Sends summaries directly to participants

### **8. ğŸ§‘â€ğŸ¤â€ğŸ§‘ Speaker Diarization**
- Identifies **who spoke when** during the meeting
- Implemented with **pyannote.audio** diarization pipeline
- **Output:** Timestamps with speaker labels (`SPEAKER_00`, `SPEAKER_01`, â€¦)
- **Next:** Merge diarization with transcripts â†’ speaker-attributed summaries

### **9. âš¡ Real-Time WebSocket Updates (Phase 3)**
- **Problem:** Eliminated inefficient polling (60 requests/minute)
- **Solution:** WebSocket connections for instant updates
- **Benefits:**
  - 97% reduction in network requests
  - 20x faster updates (<100ms vs 0-2000ms)
  - Real-time processing status (diarization, transcription, alignment)
  - Auto-reconnection with exponential backoff
- **Implementation:**
  - Backend: WebSocket manager + broadcasting during audio processing
  - Frontend: Custom `useWebSocket` React hook
  - Connection status indicator in UI
- **See:** `PHASE3_SUMMARY.md`, `PHASE3_QUICKSTART.md` for details

### **10. ğŸ¤– Automated Meeting Bot (Phase 4) - THE KILLER FEATURE**
- **Problem:** Manual meeting joining and transcription setup
- **Solution:** Automated bot that joins Google Meet and captures audio
- **Features:**
  - âœ… Automated Google Meet joining (Puppeteer)
  - âœ… Real-time audio capture (puppeteer-stream)
  - âœ… Live transcription streaming (Whisper + WebSocket)
  - âœ… Browser automation with intelligent join detection
  - âœ… Headless/visible mode for debugging
- **Architecture:**
  - Bot Engine (Node.js) â†’ Audio Capture â†’ WebSocket â†’ Backend (Python)
  - Backend processes audio with Whisper â†’ Broadcasts to Frontend
  - Seamless integration with Phase 3 WebSocket infrastructure
- **Usage:**
  ```bash
  cd bot_engine
  npm install
  npm start
  ```
- **See:** `PHASE4_QUICKSTART.md`, `bot_engine/README.md` for complete guide

---

## **ğŸ— Project Structure**

```
inclusive-meeting-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI entrypoint + WebSocket endpoints
â”‚   â”œâ”€â”€ bot_audio_processor.py   # Bot audio processing & Whisper integration
â”‚   â”œâ”€â”€ websocket_manager.py     # WebSocket connection management
â”‚   â”œâ”€â”€ pipeline_runner.py       # Orchestrates NLP pipeline
â”‚   â”œâ”€â”€ nlp_module/
â”‚   â”‚   â”œâ”€â”€ nlp_pipeline.py      # Summarization, Action items, Translation
â”‚   â”‚   â”œâ”€â”€ translate_text.py
â”‚   â”œâ”€â”€ speech_Module/           # Whisper / ASR integration
â”‚   â”œâ”€â”€ tts_Module/
â”‚   â”‚   â”œâ”€â”€ text_to_speech.py
â”‚   â”œâ”€â”€ speaker_diarization.py   # Pyannote diarization pipeline
â”‚   â””â”€â”€ output/                  # Transcripts, summaries, etc.
â”‚
â”œâ”€â”€ sign_language/               # Sign Language Recognition (Phase 4)
â”‚   â”œâ”€â”€ inference.py             # Real-time sign detection with ML model
â”‚   â”œâ”€â”€ meeting_actions.h5       # Trained LSTM model
â”‚   â”œâ”€â”€ train_model.py           # Model training script
â”‚   â””â”€â”€ MP_Data/                 # Training data for 6 gestures
â”‚
â”œâ”€â”€ bot_engine/                  # Meeting Bot (Phase 4)
â”‚   â”œâ”€â”€ bot_engine.js            # Puppeteer automation + audio capture
â”‚   â”œâ”€â”€ package.json             # Node.js dependencies
â”‚   â”œâ”€â”€ .env.example             # Configuration template
â”‚   â””â”€â”€ README.md                # Bot setup guide
â”‚
â”œâ”€â”€ frontend/                    # React.js (UI for meetings)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.jsx # WebSocket client with sign language support
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ LiveMeeting.jsx  # Live meeting with integrated sign language
â”‚   â”‚   â””â”€â”€ components/live-session/
â”‚   â”‚       â”œâ”€â”€ TranscriptFeed.jsx    # Shows sign language messages
â”‚   â”‚       â””â”€â”€ SignLanguageCam.jsx   # Camera feed with overlay
â”‚
â”œâ”€â”€ start_sign_language.ps1      # Sign language launcher (Windows)
â”œâ”€â”€ start_sign_language.sh       # Sign language launcher (Linux/Mac)
â”œâ”€â”€ start_complete_system.ps1    # Launch everything at once
â”œâ”€â”€ test_sign_language_integration.py  # Integration test
â”œâ”€â”€ SIGN_LANGUAGE_INTEGRATION.md # Complete sign language guide
â”œâ”€â”€ QUICK_REFERENCE.md           # Quick start guide
â”œâ”€â”€ DATA_FLOW_DIAGRAM.md         # Architecture diagrams
â”œâ”€â”€ test_all_features.py         # Test script to verify all features
â”œâ”€â”€ test_bot_audio.py            # Bot audio processing tests
â”œâ”€â”€ setup_bot.bat / .sh          # Bot setup scripts
â”œâ”€â”€ start_bot.bat / .sh          # Bot start scripts
â””â”€â”€ README.md
```

---

## **âš™ï¸ Installation & Setup**

### **1. Clone Repository**
```bash
git clone https://github.com/yourusername/inclusive-meeting-assistant.git
cd inclusive-meeting-assistant
```

### **2. Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate    # Linux/Mac
venv\Scripts\activate       # Windows
```

### **3. Install Dependencies**
```bash
pip install -r requirements.txt
```

**Required Libraries:**
- `transformers`
- `torch`
- `pyttsx3`
- `speechbrain`
- `pyannote.audio`
- `openai-whisper`
- `fastapi`, `uvicorn`
- `mediapipe`, `opencv-python`
- `tensorflow` / `tflite-runtime`

### **4. Set HuggingFace Token**
```bash
setx HUGGINGFACE_TOKEN "hf_xxx..."
```

### **5. Run Backend**
```bash
cd backend
uvicorn main:app --reload
```

### **6. Run Test Script**
```bash
python test_all_features.py
```

---

## **ï¿½ Quick Start (All Components)**

### **Option 1: Launch Everything at Once (Recommended)**
```powershell
# Windows
.\start_complete_system.ps1

# This starts: Backend + Frontend + Sign Language Recognition
```

### **Option 2: Manual Launch**
```powershell
# Terminal 1 - Backend
cd backend
python main.py

# Terminal 2 - Frontend
cd frontend
npm run dev

# Terminal 3 - Sign Language (optional)
.\start_sign_language.ps1
```

### **Option 3: Test Integration**
```powershell
# Test sign language integration without camera
python test_sign_language_integration.py
```

**ğŸ“š For detailed sign language setup, see [SIGN_LANGUAGE_INTEGRATION.md](SIGN_LANGUAGE_INTEGRATION.md)**  
**ğŸ“ Quick reference: [QUICK_REFERENCE.md](QUICK_REFERENCE.md)**

---

## **ğŸ—º Roadmap & Completed Phases**

### **âœ… Completed**
- **Phase 1:** Core NLP features (summarization, action items, translation, TTS)
- **Phase 2:** MongoDB + JWT Authentication ([docs](PHASE2_SUMMARY.md))
- **Phase 3:** WebSocket real-time updates ([docs](PHASE3_SUMMARY.md))
- **Phase 4:** Automated Meeting Bot + **Sign Language Integration** ([docs](PHASE4_SUMMARY.md)) ğŸ‰
  - **NEW:** ML-based sign recognition fully integrated with live chat
  - 6 basic gestures: hello, yes, no, question, thanks
  - Real-time WebSocket broadcasting
  - Special UI styling in transcript feed
- **Phase 5:** Browser-based client-side sign language detection ([docs](PHASE5_SUMMARY.md))

### **ğŸ”œ Upcoming**
- **Phase 6:** Full frontend integration with authentication
- Enhanced sign language vocabulary
- Mobile device support
- **Phase 6:** Collaborative features (multi-user editing)
- **Phase 7:** Support for Zoom, Microsoft Teams
- Merge diarization output with transcript â†’ **speaker-attributed summaries**
- Topic segmentation â†’ break meetings into themes
- Export options (PDF, Notion, etc.)
- Browser extension (Zoom / Meet integration)
- Real-time dashboard with speaker labels & sign language overlay

## **ğŸ“š Documentation**

### Phase 2: Authentication & Database
- [PHASE2_SUMMARY.md](PHASE2_SUMMARY.md) - Complete overview
- [PHASE2_QUICKSTART.md](PHASE2_QUICKSTART.md) - Quick start guide
- [PHASE2_IMPLEMENTATION.md](PHASE2_IMPLEMENTATION.md) - Technical details
- [PHASE2_ARCHITECTURE.md](PHASE2_ARCHITECTURE.md) - Architecture diagram

### Phase 3: WebSocket Real-Time
- [PHASE3_SUMMARY.md](PHASE3_SUMMARY.md) - Complete overview
- [PHASE3_QUICKSTART.md](PHASE3_QUICKSTART.md) - Testing guide
- [PHASE3_COMPARISON.md](PHASE3_COMPARISON.md) - Before/after analysis
- [PHASE3_WEBSOCKET_INTEGRATION.md](PHASE3_WEBSOCKET_INTEGRATION.md) - Technical guide

### Phase 4: Meeting Bot ğŸ¤–
- [PHASE4_INDEX.md](PHASE4_INDEX.md) - **START HERE** - Complete documentation index
- [PHASE4_QUICKSTART.md](PHASE4_QUICKSTART.md) - 5-minute setup guide
- [bot_engine/README.md](bot_engine/README.md) - Comprehensive bot guide
- [PHASE4_SUMMARY.md](PHASE4_SUMMARY.md) - Technical architecture & implementation
- [PHASE4_CHECKLIST.md](PHASE4_CHECKLIST.md) - Development & deployment checklist
