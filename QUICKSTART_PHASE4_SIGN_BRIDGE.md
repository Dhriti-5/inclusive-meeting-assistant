# ğŸš€ Quick Start: Sign Language Bridge Testing

## Prerequisites

âœ… Python 3.8+ installed  
âœ… Node.js 16+ installed  
âœ… MongoDB running (for backend)  
âœ… Webcam available  
âœ… Google Account with meeting access

---

## Step-by-Step Testing Guide

### 1ï¸âƒ£ Install Dependencies

```bash
# Backend dependencies (if not already installed)
cd backend
pip install -r ../requirements.txt
pip install requests

# Bot engine dependencies
cd ../bot_engine
npm install

# Sign language dependencies already installed from training
cd ../sign_language
# requests should be installed already
pip install requests
```

---

### 2ï¸âƒ£ Start Backend Server

**Terminal 1:**
```bash
cd backend
python start_server.py
```

**Wait for:**
```
âœ… Application started
INFO:     Uvicorn running on http://0.0.0.0:8000
```

---

### 3ï¸âƒ£ Test Backend is Ready

**Terminal 2:**
```bash
# Test health endpoint
curl http://localhost:8000/

# Should return:
# {"message":"Inclusive Meeting Assistant Backend is running."}

# Test sign language queue status
curl http://localhost:8000/api/sign-queue-status

# Should return:
# {"queue_length":0,"pending_messages":[]}
```

---

### 4ï¸âƒ£ Start Sign Language Inference

**Terminal 3:**
```bash
cd sign_language
python inference.py
```

**Wait for:**
```
âœ… Model loaded successfully!
âœ… Webcam opened successfully!
ğŸ”— Connected to backend at: http://localhost:8000
ğŸ¥ Starting inference... Press 'q' to quit.
```

**You should see:**
- OpenCV window showing your webcam feed
- Hand skeleton tracking (green lines)
- Probability bars on the left showing detected gestures

---

### 5ï¸âƒ£ Test Sign Detection (Without Bot)

1. **Make a "question" sign** in front of webcam:
   - Raise your hand with index finger pointing up
   - Hold for 1-2 seconds

2. **Check Terminal 3 output:**
   ```
   ğŸš€ SENT TO BOT: question (confidence: 0.87)
      Message: [Sign Language] ğŸ™‹ Participant has a question
   ```

3. **Verify backend received it:**
   ```bash
   # In Terminal 2
   curl http://localhost:8000/api/sign-queue-status
   
   # Should show:
   # {"queue_length":1,"pending_messages":["[Sign Language] ğŸ™‹ Participant has a question"]}
   ```

âœ… **If you see this, the sign language â†’ backend connection works!**

---

### 6ï¸âƒ£ Start Google Meet Bot

**Terminal 4:**
```bash
cd bot_engine
npm start
```

**Wait for:**
```
ğŸ¤– Initializing Google Meet Bot...
ğŸš€ Launching Chrome...
âœ… Browser launched successfully
ğŸ” Logging into Google Account...
âœ… Logged in successfully
ğŸ¯ Joining meeting...
âœ… Joined meeting successfully
ğŸ”Œ Connecting to backend WebSocket...
âœ… WebSocket connected
ğŸ¤ Starting audio capture...
âœ… Audio capture started
ğŸ¤Ÿ Starting Sign Language Command Poller...
   Polling interval: 1000ms
ğŸ‰ Bot is fully operational!
   - Audio transcription: âœ…
   - Sign language bridge: âœ…
```

**You should see:**
- Chrome browser window (if not headless)
- Bot joining your Google Meet
- Bot in the participant list

---

### 7ï¸âƒ£ End-to-End Test

1. **Join the same Google Meet** (from your regular browser)

2. **Position yourself** in front of the webcam running inference

3. **Make a "hello" sign:**
   - Wave your hand
   - Or make a fist and extend thumb (ASL hello)

4. **Watch the magic happen:**

   **Terminal 3 (Inference):**
   ```
   ğŸš€ SENT TO BOT: hello (confidence: 0.92)
      Message: [Sign Language] ğŸ‘‹ Participant says Hello!
   ```

   **Terminal 4 (Bot):**
   ```
   âœï¸ Sign Language Command: [Sign Language] ğŸ‘‹ Participant says Hello!
      ğŸ“‚ Opened chat panel
      âœ… Message sent to chat: "[Sign Language] ğŸ‘‹ Participant says Hello!"
   ```

   **Google Meet Chat:**
   ```
   Bot: [Sign Language] ğŸ‘‹ Participant says Hello!
   ```

ğŸ‰ **SUCCESS! The full pipeline is working!**

---

## 8ï¸âƒ£ Test All Gestures

Try each of these signs and verify they appear in chat:

| Gesture | Expected Chat Message |
|---------|----------------------|
| ğŸ™‹ Question | [Sign Language] ğŸ™‹ Participant has a question |
| ğŸ‘‹ Hello | [Sign Language] ğŸ‘‹ Participant says Hello! |
| âœ… Yes | [Sign Language] âœ… Participant agrees |
| âŒ No | [Sign Language] âŒ Participant disagrees |
| ğŸ™ Thanks | [Sign Language] ğŸ™ Participant says Thank You |

---

## ğŸ› Troubleshooting

### Issue: Inference can't connect to backend

**Error:** `âš ï¸ Backend not reachable at http://localhost:8000`

**Fix:**
1. Check backend is running (Terminal 1)
2. Test: `curl http://localhost:8000/`
3. Check firewall settings

---

### Issue: Bot not typing in chat

**Check:**
1. Is bot still in meeting? (check participant list)
2. Is chat enabled in meeting settings?
3. Check bot terminal for errors

**Debug:**
```bash
# Check if commands are queuing
curl http://localhost:8000/api/sign-queue-status

# If queue is stuck, clear it
curl -X POST http://localhost:8000/api/clear-sign-queue
```

---

### Issue: Detection not stable

**Problem:** Flickering between different gestures

**Solutions:**
1. Ensure good lighting
2. Clear background
3. Hold gesture steady for 2-3 seconds
4. Check confidence threshold in inference.py:
   ```python
   threshold = 0.8  # Try 0.9 for stricter detection
   ```

---

### Issue: Too many duplicate messages

**Problem:** Same message sent multiple times

**Fix:** Increase cooldown in inference.py:
```python
SEND_COOLDOWN = 5  # Wait 5 seconds between same word
```

---

## ğŸ“Š Performance Check

Good performance indicators:

âœ… **Inference FPS:** 25-30 FPS  
âœ… **Detection Latency:** < 2 seconds  
âœ… **API Response:** < 100ms  
âœ… **Bot Polling:** Every 1 second  
âœ… **End-to-end:** 1-3 seconds from gesture to chat

---

## ğŸ§ª Advanced Testing

### Test with Multiple Signs in Sequence

1. Make "hello" gesture â†’ Wait for message in chat
2. Make "yes" gesture â†’ Wait for message
3. Make "question" gesture â†’ Wait for message

**Expected:** 3 separate messages in chat

---

### Test Cooldown Behavior

1. Make "hello" gesture
2. Immediately make "hello" again (within 3 seconds)

**Expected:** Only one message sent (cooldown prevents duplicate)

---

### Test Confidence Filtering

1. Make a half-gesture (unclear sign)
2. Check Terminal 3

**Expected:** 
```
â„¹ï¸ Ignored: Low confidence or idle state
```

No message sent to bot.

---

## ğŸ“¸ Screenshot Checklist

Document your successful test with screenshots of:

1. âœ… All 4 terminals running
2. âœ… OpenCV window showing hand tracking
3. âœ… Google Meet with bot in participant list
4. âœ… Chat messages from bot
5. âœ… Backend terminal showing "Sign Detected" logs

---

## ğŸ“ What You've Built

Congratulations! You now have:

âœ… Real-time sign language detection  
âœ… API integration between Python and Node.js  
âœ… Automated chat bot for accessibility  
âœ… End-to-end pipeline from webcam to chat  
âœ… Production-ready error handling  
âœ… Scalable architecture with queue system  

---

## ğŸš€ Next Steps

1. **Train more gestures** - Expand vocabulary
2. **Deploy to cloud** - Make it accessible remotely
3. **Add frontend UI** - Real-time gesture visualization
4. **Improve accuracy** - Collect more training data
5. **Add analytics** - Track gesture usage statistics

---

**Need Help?** Check [PHASE4_SIGN_LANGUAGE_BRIDGE.md](PHASE4_SIGN_LANGUAGE_BRIDGE.md) for detailed documentation.
